---
# yaml-language-server: $schema=https://raw.githubusercontent.com/budimanjojo/talhelper/master/pkg/config/schemas/talconfig.json
# renovate: datasource=docker depName=ghcr.io/siderolabs/installer
talosVersion: v1.9.5
# renovate: datasource=docker depName=ghcr.io/siderolabs/kubelet
kubernetesVersion: v1.32.3

clusterName: dormammu
endpoint: https://192.168.9.25:6443

clusterPodNets:
  - &podNet "172.16.0.0/16"
clusterSvcNets:
  - &svcNet "172.17.0.0/16"

additionalApiServerCertSans: &sans
  - "10.38.55.200"
  - "192.168.9.25"
  - "10.0.30.28"
  - "10.40.40.41"
  - "127.0.0.1"
  - "${CLOUD_IP}"
  - "posey"
  - "bonds"
  - "madbum"
additionalMachineCertSans: *sans

# Disable built-in Flannel to use Cilium
cniConfig:
  name: none

nodes:
  - hostname: "bonds"
    ipAddress: "192.168.9.25"
    installDiskSelector:
      wwid: eui.000000000000000100a07523458d4f6c
    machineSpec:
      secureboot: false
      mode: metal
    nodeLabels:
      location: "apartment"
      nvidia.com/gpu.present: "true"
      nvidia.com/gpu.model: "4090"
    kernelModules:
      - name: nvidia
      - name: nvidia_uvm
      - name: nvidia_drm
      - name: nvidia_modeset
      - name: zfs
    schematic:
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/nvidia-container-toolkit-production
            - siderolabs/nonfree-kmod-nvidia-production
            - siderolabs/zfs
            - siderolabs/amd-ucode
    controlPlane: true
    networkInterfaces:
      - deviceSelector:
          hardwareAddr: "a0:36:9f:09:4c:8f"
        addresses:
          - "192.168.9.25/24"
        routes:
          - network: "0.0.0.0/0"
            gateway: "192.168.9.1"
        mtu: 1500
      - deviceSelector:
          hardwareAddr: a0:36:9f:09:4c:8e
        ignore: true
      - deviceSelector:
          hardwareAddr: a8:a1:59:dd:87:18
        ignore: true
    patches:
      - |-
        machine:
          install:
            extraKernelArgs:
              - amd_pstate=active
          udev:
            rules:
              - SUBSYSTEM=="block", ENV{DEVTYPE}=="disk", ATTR{queue/scheduler}="none"
              - SUBSYSTEMS=="usb", ATTRS{idVendor}=="10c4", ATTRS{idProduct}=="ea60", GROUP="20", MODE="0660", TAG+="uaccess"
          disks:
            - device: /dev/disk/by-id/nvme-eui.00253841414031f0
              partitions:
                - mountpoint: /var/lib/csi-local-hostpath
          kubelet:
            nodeIP:
              validSubnets:
                - "192.168.9.0/24"
            extraMounts:
              - destination: /var/lib/csi-local-hostpath
                type: bind
                source: /var/lib/csi-local-hostpath
                options: ["bind", "rshared", "rw"]
  - hostname: "posey"
    ipAddress: "10.0.30.28"
    installDiskSelector:
      model: "KINGSTON SNV2S1000G"
    machineSpec:
      secureboot: false
      mode: metal
    nodeLabels:
      location: "house"
      nvidia.com/gpu.present: "true"
      nvidia.com/gpu.model: "p400"
    kernelModules:
      - name: nvidia
      - name: nvidia_uvm
      - name: nvidia_drm
      - name: nvidia_modeset
    schematic:
      customization:
        systemExtensions:
          officialExtensions:
            - siderolabs/nvidia-container-toolkit-production
            - siderolabs/nonfree-kmod-nvidia-production
            - siderolabs/amd-ucode
    controlPlane: false
    nodeTaints:
      node-role.kubernetes.io/control-plane: NoSchedule
    networkInterfaces:
      - interface: bond0
        dhcp: false
        mtu: 1500
        addresses:
          - 10.0.30.28/24
        routes:
          - network: "0.0.0.0/0"
            gateway: "10.0.30.1"
        bond:
          mode: 802.3ad
          deviceSelectors:
            - hardwareAddr: "c4:62:37:01:8c:cf"
            - hardwareAddr: "c4:62:37:01:8c:ce"
      - deviceSelector:
          hardwareAddr: a8:5e:45:9c:d0:99
        ignore: true
    patches:
      - |-
        machine:
          install:
            extraKernelArgs:
              - amd_pstate=active
          kubelet:
            nodeIP:
              validSubnets:
                - "10.0.30.0/24"
  - hostname: "madbum"
    ipAddress: "10.10.10.3"
    installDiskSelector:
      size: ">= 10GB"
    machineSpec:
      secureboot: false
      mode: cloud
    nodeLabels:
      location: "cloud"
    controlPlane: false
    nodeTaints:
      node-role.kubernetes.io/control-plane: NoSchedule
    networkInterfaces:
      - interface: eth0
        dhcp: false
        addresses:
          - 10.40.40.41/24
        routes:
          - network: "0.0.0.0/0"
            gateway: "10.40.40.1"
        mtu: 1500
      - interface: wg0
        addresses:
          - 10.10.10.3/24
        mtu: 1420
        routes:
          - network: 192.168.9.0/24
            gateway: 10.10.10.1
          - network: 10.0.0.0/16
            gateway: 10.10.10.2
        wireguard:
          privateKey: ${MADBUM_WG_PRIVATE_KEY}
          listenPort: 51820
          peers:
            - publicKey: ${APARTMENT_WG_PUBKEY}
              endpoint: ${APARTMENT_WG_ENDPOINT}
              allowedIPs:
                - 192.168.9.0/24
                - 10.10.10.1/32
              persistentKeepaliveInterval: 10s
            - publicKey: ${OFFICE_WG_PUBKEY}
              endpoint: ${OFFICE_WG_ENDPOINT}
              allowedIPs:
                - 10.0.0.0/16
                - 10.10.10.2/32
              persistentKeepaliveInterval: 10s
    patches:
      - |-
        machine:
          kubelet:
            nodeIP:
              validSubnets:
                - "10.10.10.0/24"

# Global patches
patches:
  - "@./patches/global/machine-files.yaml"
  - "@./patches/global/machine-kubelet.yaml"
  - "@./patches/global/machine-network.yaml"
  - "@./patches/global/machine-sysctls.yaml"
  - "@./patches/global/machine-time.yaml"
  - "@./patches/global/machine-install.yaml"
  - "@./patches/global/machine-kernel.yaml"
  - "@./patches/global/machine-features.yaml"

# Controller patches
controlPlane:
  patches:
    - "@./patches/controller/admission-controller-patch.yaml"
    - "@./patches/controller/cluster.yaml"
